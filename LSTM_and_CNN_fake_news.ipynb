{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM and CNN fake news.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashdusing/100-Days-Of-ML-Code/blob/master/LSTM_and_CNN_fake_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nNfZa4ffSR6Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def load_kagglefakenews():\n",
        "    #load training data and put into arrays\n",
        "    df = pd.read_csv('train.csv', encoding='utf8', engine='python', error_bad_lines=False) # be sure to point to wherever you put your file\n",
        "    train_data = df['text'].values.tolist() #'text' column contains articles\n",
        "    train_labels = df['label'].values.tolist() #'label' column contains labels\n",
        "\n",
        "    #Randomly shuffle data and labels together\n",
        "    combo = list(zip(train_data, train_labels))\n",
        "    random.shuffle(combo)\n",
        "    train_data, train_labels = zip(*combo)\n",
        "    del df #clear up memory\n",
        "\n",
        "    return np.asarray(train_data).tolist(), np.asarray(train_labels).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "loLciiMzUIYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b5ec565-bda7-4f29-a505-03bb74876213"
      },
      "cell_type": "code",
      "source": [
        "train_data, train_labels = load_kagglefakenews()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skipping line 12954: unexpected end of data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5xtkqROUUO_D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a11f2de3-067c-4f9e-d412-73300d49a7fe"
      },
      "cell_type": "code",
      "source": [
        "print(train_labels[:5])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oAKq3mqiV4-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "f438ce50-8fcf-46ba-ea5f-3d4390594d3c"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "\n",
        "MAX_NB_WORDS=50000 #dictionary size\n",
        "MAX_SEQUENCE_LENGTH=1500 #max word length of each individual article\n",
        "EMBEDDING_DIM=300 #dimensionality of the embedding vector (50, 100, 200, 300)\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
        "\n",
        "def tokenize_trainingdata(texts, labels):\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    pickle.dump(tokenizer, open('Models/tokenizer.p', 'wb'))\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    print(sequences[0])\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    labels = to_categorical(labels, num_classes=len(set(labels)))\n",
        "\n",
        "    return data, labels, word_index\n",
        "\n",
        "#and run it\n",
        "X, Y, word_index = tokenize_trainingdata(train_data, train_labels)   "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17574, 24, 6219, 138, 6, 99, 7340, 1670, 1, 16196, 5525, 12195, 5452, 159, 1082, 10580, 28, 5453, 15, 189, 3, 1, 1849, 322, 866, 5, 208, 7, 5454, 75, 736, 2419, 139, 261, 4, 534, 772, 1750, 9, 123, 114, 197, 1849, 66, 4681, 119, 94, 347, 39, 12196, 3073, 2, 123, 114, 9, 510, 18, 10580, 10, 389, 577, 145, 2, 2777, 12, 3450, 37, 54, 6572, 39, 831, 627, 2, 475, 2, 5146, 3055, 4681, 1, 119, 10, 479, 123, 114, 68, 2239, 166, 39, 2567, 897, 568, 4177, 3073, 2, 1, 123, 114, 9, 5, 510, 136, 93, 1026, 10, 1, 911, 10, 491, 610, 66, 3073, 11335, 39, 10, 25, 3451, 485, 297, 39, 13, 5, 22428, 1000, 3, 627, 11, 179, 2, 5, 331, 18, 3073, 150, 38, 911, 380, 1, 93, 1237, 2, 1245, 21, 1, 9568, 1167, 11, 9494, 1, 119, 700, 3073, 590, 5, 331, 22429, 39, 297, 7747, 2239, 39, 8, 5, 644, 1000, 3, 1, 80, 3, 627, 42, 41, 162, 516, 2, 2120, 1, 196, 4453, 4, 821, 6, 69, 924, 1, 721, 1762, 173, 1, 98, 87, 4, 627, 8, 453, 10, 1495, 1630, 1495, 966, 4, 5, 1495, 7035, 11070, 893, 7, 68, 2239, 39, 4, 33, 41, 483, 2, 4846, 1, 2828, 2240, 173, 627, 4, 1, 98, 87, 4, 727, 14, 2, 54, 5648, 11, 16, 280, 835, 66, 14, 13, 5235, 659, 27, 384, 7, 166, 39, 13, 862, 863, 191, 18, 1444, 41429, 5526, 2748, 14543, 13449, 32, 8568, 9, 18000, 4, 1, 5526, 80, 4847, 38132, 29896, 64, 11335, 39, 2614, 160, 2, 13672, 68, 2239, 166, 39, 25, 237, 4, 51, 1, 96, 50, 32, 20, 292, 283, 1686, 1, 105, 14, 8, 1, 1915, 1151, 11, 2614, 666, 516, 5147, 4, 247, 63, 1919, 69, 5968, 4, 2524, 52, 19, 1, 7683, 3, 2890, 4, 4243, 788, 6349, 34, 26, 68, 11, 29896, 280, 45640, 520, 11595, 27188, 5526, 579, 11335, 39, 10, 425, 5821, 2, 1, 68, 3, 1, 5046, 4, 1043, 401, 204, 166, 499, 39, 11, 876, 26081, 8775, 2651, 2642, 153, 119, 45, 727, 29, 241, 2, 1, 93, 80, 1839, 38133, 726, 8, 5, 3422, 1167, 9, 627, 2, 1066, 4122, 58, 3591, 2, 31423, 10, 1, 603, 3, 6009, 4190, 6, 1, 947, 3, 1, 138, 165, 5, 1293, 3137, 2, 69, 196, 4, 1, 335, 3, 69, 837, 6394, 8, 1, 68, 23990, 5455, 15, 14, 1021, 6, 25, 2146, 4, 7, 2503, 140, 22, 69, 121, 33312, 1346, 4, 374, 1, 1683, 3, 1, 1843, 80, 8, 75, 11]\n",
            "Found 189972 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cVOeRg1UXloj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = X[:int(len(X)*0.9)]\n",
        "train_labels = Y[:int(len(X)*0.9)]\n",
        "test_data = X[int(len(X)*0.9):int(len(X)*0.95)]\n",
        "test_labels = Y[int(len(X)*0.9):int(len(X)*0.95)]\n",
        "valid_data = X[int(len(X)*0.95):]\n",
        "valid_labels = Y[int(len(X)*0.95):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2zjcZD5Xued",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48c55c99-a12e-4fd4-abf2-1affd60797f6"
      },
      "cell_type": "code",
      "source": [
        "def load_embeddings(word_index, embeddingsfile='wordEmbeddings/glove.6B.%id.txt' %EMBEDDING_DIM):\n",
        "    embeddings_index = {}\n",
        "    f = open(embeddingsfile, 'r', encoding='utf8')\n",
        "    for line in f:\n",
        "        #here we parse the data from the file\n",
        "        values = line.split(' ') #split the line by spaces\n",
        "        word = values[0] #each line starts with the word\n",
        "        coefs = np.asarray(values[1:], dtype='float32') #the rest of the line is the vector\n",
        "        embeddings_index[word] = coefs #put into embedding dictionary\n",
        "    f.close()\n",
        "\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    embedding_layer = Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=False)\n",
        "    return embedding_layer\n",
        "    \n",
        "#and build the embedding layer\n",
        "embedding_layer = load_embeddings(word_index)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4eyXZYAWYP9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import Sequential, Model, Input\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dense, GlobalAveragePooling1D, Dropout, LSTM, CuDNNLSTM, RNN, SimpleRNN, Conv2D, GlobalMaxPooling1D\n",
        "from keras import callbacks\n",
        "\n",
        "def baseline_model(sequence_input, embedded_sequences, classes=2):\n",
        "    x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
        "    x = MaxPooling1D(5)(x)\n",
        "    x = Conv1D(128, 3, activation='relu')(x)\n",
        "    x = MaxPooling1D(5)(x)\n",
        "    x = Conv1D(256, 2, activation='relu')(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(2048, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    preds = Dense(classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FqZWcUifdGCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1522
        },
        "outputId": "1308ef4e-44bb-469a-be05-935f885af1ac"
      },
      "cell_type": "code",
      "source": [
        "#put embedding layer into input of the model\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "model = baseline_model(sequence_input, embedded_sequences, classes=2)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(train_data, train_labels, validation_data=(valid_data, valid_labels), epochs=25, batch_size=64)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1500)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 1500, 300)         56991900  \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1496, 64)          96064     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 299, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 297, 128)          24704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 59, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 58, 256)           65792     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2048)              526336    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 58,754,910\n",
            "Trainable params: 1,763,010\n",
            "Non-trainable params: 56,991,900\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 11656 samples, validate on 648 samples\n",
            "Epoch 1/25\n",
            "11656/11656 [==============================] - 11s 957us/step - loss: 0.3731 - acc: 0.8240 - val_loss: 0.2602 - val_acc: 0.8750\n",
            "Epoch 2/25\n",
            "11656/11656 [==============================] - 8s 665us/step - loss: 0.1347 - acc: 0.9509 - val_loss: 0.1296 - val_acc: 0.9491\n",
            "Epoch 3/25\n",
            "11656/11656 [==============================] - 8s 665us/step - loss: 0.0631 - acc: 0.9796 - val_loss: 0.1309 - val_acc: 0.9568\n",
            "Epoch 4/25\n",
            "11656/11656 [==============================] - 8s 667us/step - loss: 0.0295 - acc: 0.9905 - val_loss: 0.1802 - val_acc: 0.9522\n",
            "Epoch 5/25\n",
            "11656/11656 [==============================] - 8s 668us/step - loss: 0.0179 - acc: 0.9943 - val_loss: 0.1177 - val_acc: 0.9707\n",
            "Epoch 6/25\n",
            "11656/11656 [==============================] - 8s 666us/step - loss: 0.0105 - acc: 0.9972 - val_loss: 0.1369 - val_acc: 0.9707\n",
            "Epoch 7/25\n",
            "11656/11656 [==============================] - 8s 667us/step - loss: 0.0139 - acc: 0.9958 - val_loss: 0.1364 - val_acc: 0.9722\n",
            "Epoch 8/25\n",
            "11656/11656 [==============================] - 8s 668us/step - loss: 0.0051 - acc: 0.9987 - val_loss: 0.1241 - val_acc: 0.9769\n",
            "Epoch 9/25\n",
            "11656/11656 [==============================] - 8s 668us/step - loss: 0.0051 - acc: 0.9987 - val_loss: 0.1757 - val_acc: 0.9676\n",
            "Epoch 10/25\n",
            "11656/11656 [==============================] - 8s 670us/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.1528 - val_acc: 0.9691\n",
            "Epoch 11/25\n",
            "11656/11656 [==============================] - 8s 669us/step - loss: 0.0051 - acc: 0.9990 - val_loss: 0.1765 - val_acc: 0.9645\n",
            "Epoch 12/25\n",
            "11656/11656 [==============================] - 8s 669us/step - loss: 0.0027 - acc: 0.9997 - val_loss: 0.1329 - val_acc: 0.9753\n",
            "Epoch 13/25\n",
            "11656/11656 [==============================] - 8s 668us/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1510 - val_acc: 0.9707\n",
            "Epoch 14/25\n",
            "11656/11656 [==============================] - 8s 669us/step - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1543 - val_acc: 0.9691\n",
            "Epoch 15/25\n",
            "11656/11656 [==============================] - 8s 667us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1622 - val_acc: 0.9691\n",
            "Epoch 16/25\n",
            "11656/11656 [==============================] - 8s 670us/step - loss: 0.0019 - acc: 0.9998 - val_loss: 0.1656 - val_acc: 0.9722\n",
            "Epoch 17/25\n",
            "11656/11656 [==============================] - 8s 669us/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1602 - val_acc: 0.9722\n",
            "Epoch 18/25\n",
            "11656/11656 [==============================] - 8s 668us/step - loss: 0.0022 - acc: 0.9998 - val_loss: 0.1645 - val_acc: 0.9722\n",
            "Epoch 19/25\n",
            "11656/11656 [==============================] - 8s 668us/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.1664 - val_acc: 0.9722\n",
            "Epoch 20/25\n",
            "11656/11656 [==============================] - 8s 669us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1681 - val_acc: 0.9691\n",
            "Epoch 21/25\n",
            "11656/11656 [==============================] - 8s 669us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1942 - val_acc: 0.9738\n",
            "Epoch 22/25\n",
            "11656/11656 [==============================] - 8s 669us/step - loss: 0.0023 - acc: 0.9998 - val_loss: 0.1831 - val_acc: 0.9738\n",
            "Epoch 23/25\n",
            "11656/11656 [==============================] - 8s 667us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.1891 - val_acc: 0.9738\n",
            "Epoch 24/25\n",
            "11656/11656 [==============================] - 8s 669us/step - loss: 0.0022 - acc: 0.9998 - val_loss: 0.1933 - val_acc: 0.9738\n",
            "Epoch 25/25\n",
            "11656/11656 [==============================] - 8s 670us/step - loss: 0.0021 - acc: 0.9998 - val_loss: 0.2151 - val_acc: 0.9722\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc84ef9c4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "-1GoJC6tdwfA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "abfc0acd-2678-4b58-d888-d05ad8ff52ba"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data, test_labels)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "648/648 [==============================] - 0s 611us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18419567021855085, 0.9783950617283951]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "YCRyuphtelIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}